# -*- coding: utf-8 -*-
"""KSADataProcessor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y9ZhuH_m8DGyKFb8227jnNrbnQjE_IpH
"""

!pip install PyPDF2 nltk pandas scikit-learn transformers torch

!pip install PyPDF2 nltk pandas google-colab

# -*- coding: utf-8 -*-
import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, chi2
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE


from google.colab import drive

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

class KSADataProcessor:
    def __init__(self, dataset_path):
        # Load the dataset
        self.data = pd.read_csv(dataset_path)

    def preprocess_text(self, text):
        """
        Advanced text preprocessing method
        """
        # Convert to lowercase
        text = text.lower()

        # Remove special characters and digits
        text = re.sub(r'[^a-zA-Z\s]', '', text)

        # Tokenization
        tokens = word_tokenize(text)

        # Remove stopwords
        stop_words = set(stopwords.words('english'))
        tokens = [token for token in tokens if token not in stop_words]

        return ' '.join(tokens)

    def text_augmentation(self, text, n_augmentations=2):
        """
        Perform text augmentation using synonym replacement
        """
        from nltk.corpus import wordnet
        import random

        words = text.split()
        augmented_texts = [text]

        for _ in range(n_augmentations):
            new_words = words.copy()
            random_word_index = random.randint(0, len(words)-1)

            synonyms = []
            for syn in wordnet.synsets(words[random_word_index]):
                for lemma in syn.lemmas():
                    synonyms.append(lemma.name())

            if synonyms:
                synonym = random.choice(synonyms)
                new_words[random_word_index] = synonym
                augmented_texts.append(' '.join(new_words))

        return augmented_texts

    def prepare_data(self):
        """
        Prepare data with preprocessing and augmentation
        """
        # Preprocess text
        self.data['Preprocessed_Text'] = self.data['Text'].apply(self.preprocess_text)

        # Augment data
        augmented_data = []
        for text in self.data['Preprocessed_Text']:
            augmented_texts = self.text_augmentation(text)
            augmented_data.extend(augmented_texts)

        # Create augmented dataframe
        augmented_df = pd.DataFrame({
            'Text': augmented_data,
            'Label': np.repeat(self.data['Label'], 3)  # Repeat labels for augmented texts
        })

        # Combine original and augmented data
        self.data = pd.concat([self.data, augmented_df], ignore_index=True)

        # Encode labels
        label_encoder = LabelEncoder()
        self.data['Encoded_Label'] = label_encoder.fit_transform(self.data['Label'])

        return self.data

    def feature_engineering(self, X_train_tfidf):
        """
        Advanced feature selection
        """
        # Select top features using chi-squared test
        selector = SelectKBest(chi2, k=1000)
        X_new = selector.fit_transform(X_train_tfidf, self.y_train)

        return X_new

    def train_models(self):
        """
        Train multiple models with advanced techniques
        """
        # Prepare data
        self.prepare_data()

        # Split data
        X = self.data['Preprocessed_Text']
        y = self.data['Encoded_Label']
        X_train, X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )

        # TF-IDF Vectorization with enhanced parameters
        vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words='english',
            max_df=0.7,
            min_df=2
        )
        X_train_tfidf = vectorizer.fit_transform(X_train)
        X_test_tfidf = vectorizer.transform(X_test)

        # Apply feature selection
        X_train_tfidf = self.feature_engineering(X_train_tfidf)

        # Handle class imbalance
        smote = SMOTE(random_state=42)
        X_train_resampled, y_train_resampled = smote.fit_resample(X_train_tfidf, self.y_train)

        # Create ensemble model
        ensemble_model = VotingClassifier(
            estimators=[
                ('svm', SVC(probability=True)),
                ('nb', MultinomialNB()),
                ('lr', LogisticRegression())
            ],
            voting='soft'
        )

        # Train ensemble model
        ensemble_model.fit(X_train_resampled, y_train_resampled)

        # Stratified Cross-Validation
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        cv_scores = cross_val_score(ensemble_model, X_train_resampled, y_train_resampled, cv=cv, scoring='f1_macro')

        print("Cross-Validation Scores:", cv_scores)
        print("Mean CV Score:", cv_scores.mean())

        # Predictions and Evaluation
        y_pred = ensemble_model.predict(X_test_tfidf)
        print("\nClassification Report:")
        print(classification_report(self.y_test, y_pred))

        print("\nConfusion Matrix:")
        print(confusion_matrix(self.y_test, y_pred))

        return ensemble_model, vectorizer

